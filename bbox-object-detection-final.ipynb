{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "import editdistance\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gc\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam,Adagrad,SGD\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import tqdm\n",
    "from torch.nn import functional as fnn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, Sequential, Conv2d, AvgPool2d, GRU, Linear\n",
    "from torch.nn.functional import ctc_loss, log_softmax\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.transforms import *\n",
    "\n",
    "from itertools import chain\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "import torch.distributed as dist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from string import digits, ascii_uppercase\n",
    "\n",
    "#import utils\n",
    "import math \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сид\n",
    "\n",
    "SEED = 1489\n",
    "\n",
    "\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определение девайса\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = \"./data/test/\" \n",
    "TRAIN_PATH = \"./data/train/\"\n",
    "SUBMISSION_PATH = \"./data/submission.csv\"\n",
    "TRAIN_INFO = \"./data/train.json\"\n",
    "\n",
    "IMAGE_WIDTH = 412\n",
    "IMAGE_HEIGHT = 412\n",
    "\n",
    "#IMAGE_WIDTH = 512\n",
    "#IMAGE_HEIGHT = 512\n",
    "\n",
    "\n",
    "#VAL_SIZE = 0.3\n",
    "VAL_SIZE = 0.05\n",
    "\n",
    "N_ITER = 2\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "BATCH_SIZE_VAL = 8\n",
    "LR = 3e-5\n",
    "\n",
    "\n",
    "COOR_COUNT = 4\n",
    "\n",
    "\n",
    "EXP_NAME = \"resnet34\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = open(TRAIN_INFO, \"r\")\n",
    "\n",
    "train = json_normalize(json.load(file_object))\n",
    "\n",
    "test = pd.read_csv(SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['box'] = train.nums.apply(lambda x: list(chain.from_iterable(x[0]['box'])))\n",
    "train['text'] = train.nums.apply(lambda x: x[0]['text'])\n",
    "train['file'] = train.file.apply(lambda x: x.split(\"/\")[1])\n",
    "\n",
    "train.drop(['nums'], axis =1, inplace = True)\n",
    "\n",
    "test['text'] = test.file_name.apply(lambda x: x.split(\"/\")[1])\n",
    "\n",
    "test.drop(['file_name'], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.head(25631)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names = train.file.values\n",
    "shapes = [cv2.imread(os.path.join(TRAIN_PATH, name), 0).shape for name in img_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['shapes'] = shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(x):\n",
    "    for i in range(len(x['box'])):\n",
    "        if i % 2 == 0:\n",
    "            x['box'][i] = (float(x['box'][i])/x['shapes'][1]) *  IMAGE_WIDTH\n",
    "        else:\n",
    "            x['box'][i] = (float(x['box'][i])/x['shapes'][0]) * IMAGE_HEIGHT\n",
    "\n",
    "    return x\n",
    "\n",
    "train = train.apply(lambda x: replace(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['xmin'] = train.apply(lambda x: min(x['box'][0], int(x['box'][6])), axis=1)\n",
    "train['xmax'] = train.apply(lambda x: max(x['box'][2], int(x['box'][4])), axis=1) \n",
    "train['ymin'] = train.apply(lambda x: min(x['box'][1], int(x['box'][3])), axis=1) \n",
    "train['ymax'] = train.apply(lambda x: max(x['box'][5], int(x['box'][7])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[((train.ymax <= train.ymin + 5) | (train.xmax <= train.xmin + 5))]\n",
    "\n",
    "train = train[~((train.ymax <= train.ymin + 5) | (train.xmax <= train.xmin + 5))]\n",
    "train = train[~((train.xmin < 0 )| (train.ymin < 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_images = np.random.choice(train.file.unique(), size=int(VAL_SIZE * train.file.nunique()), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = train[train.file.isin(valid_images)]\n",
    "\n",
    "train_set = train[~train.file.isin(valid_images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_set.shape, train_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = test.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN prepapation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, IMAGE_DIR, IMAGE_WIDTH, IMAGE_HEIGHT, data,\n",
    "                 transform=transforms.Compose([ToTensor(), \n",
    "                                               Normalize(\n",
    "                                                   mean=[0.485, 0.456, 0.406],\n",
    "                                                   std=[0.229, 0.224, 0.225]\n",
    "                                               )])):\n",
    "        self.IMAGE_DIR = IMAGE_DIR\n",
    "        self.IMAGE_WIDTH = IMAGE_WIDTH\n",
    "        self.IMAGE_HEIGHT = IMAGE_HEIGHT\n",
    "        \n",
    "        \n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "\n",
    "    def num_classes(self):\n",
    "        return len(self.class2index)\n",
    "\n",
    "    \n",
    "    def __len__(self, ):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "              \n",
    "        def get_boxes(obj):\n",
    "            boxes = [[obj[f] for f in ['xmin', 'ymin', 'xmax', 'ymax'] ]]\n",
    "            return torch.as_tensor(boxes, dtype=torch.float)\n",
    "\n",
    "\n",
    "        def get_areas(obj):\n",
    "            areas = [(obj['xmax'] - obj['xmin']) * (obj['ymax'] - obj['ymin']) ]\n",
    "            return torch.as_tensor(areas, dtype=torch.int64)\n",
    "\n",
    "        img_name = self.data.iloc[idx]['file']\n",
    "        \n",
    "        path = os.path.join(self.IMAGE_DIR, img_name)\n",
    "\n",
    "        img = cv2.imread(path)\n",
    "        \n",
    "        shapes  = img.shape\n",
    "        \n",
    "\n",
    "        \n",
    "        img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT)) \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        img_bbox = self.data.iloc[idx]['box']#.copy()\n",
    "        \n",
    "        #print(img_bbox, shapes, img_name)\n",
    "\n",
    "        obj = {}        \n",
    "        \n",
    "        obj['xmin'] = np.min([int(img_bbox[0]), int(img_bbox[6])])\n",
    "        obj['xmax'] = np.max([int(img_bbox[2]), int(img_bbox[4])])\n",
    "        obj['ymin'] = np.min([int(img_bbox[1]), int(img_bbox[3])])\n",
    "        obj['ymax'] = np.max([int(img_bbox[5]), int(img_bbox[7])])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(img)\n",
    "            \n",
    "       \n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = get_boxes(obj)\n",
    "\n",
    "        target['labels'] = torch.ones((1,), dtype=torch.int64)#torch.as_tensor(1, dtype=torch.int64)\n",
    "        target['image_id'] = torch.as_tensor([idx], dtype=torch.int64)\n",
    "        target['area'] = get_areas(obj)\n",
    "        target['iscrowd'] = torch.ones((1,), dtype=torch.int64)#get_iscrowds(annot)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class ShapeDatasetTest(Dataset):\n",
    "\n",
    "    def __init__(self, IMAGE_DIR, IMAGE_WIDTH, IMAGE_HEIGHT, data,\n",
    "                 transform=transforms.Compose([ToTensor(),\n",
    "                                               Normalize(\n",
    "                                                   mean=[0.485, 0.456, 0.406],\n",
    "                                                   std=[0.229, 0.224, 0.225]\n",
    "                                               ) ])):\n",
    "        self.IMAGE_DIR = IMAGE_DIR\n",
    "        self.IMAGE_WIDTH = IMAGE_WIDTH\n",
    "        self.IMAGE_HEIGHT = IMAGE_HEIGHT\n",
    "        \n",
    "        \n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def num_classes(self):\n",
    "        return len(self.class2index)\n",
    "\n",
    "    \n",
    "    def __len__(self, ):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        img_name = self.data.iloc[idx]['text']\n",
    "        \n",
    "        path = os.path.join(self.IMAGE_DIR, img_name)\n",
    "\n",
    "        img = cv2.imread(path)\n",
    "        \n",
    "        shapes  = img.shape\n",
    "\n",
    "        img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT)) \n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(img)\n",
    "        return image \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ShapeDataset(TRAIN_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, train_set)\n",
    "valid_data = ShapeDataset(TRAIN_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, valid_set)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data  = ShapeDatasetTest(TEST_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "\n",
    "    train_data, batch_size=4, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "dataloader_valid = DataLoader(\n",
    "    valid_data, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 2\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "#optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler\n",
    "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=5,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1. / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            #sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return metric_logger\n",
    "\n",
    "\n",
    "def _get_iou_types(model):\n",
    "    model_without_ddp = model\n",
    "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "        model_without_ddp = model.module\n",
    "    iou_types = [\"bbox\"]\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
    "        iou_types.append(\"segm\")\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
    "        iou_types.append(\"keypoints\")\n",
    "    return iou_types\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "            outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, dataloader_train, device, epoch, print_freq=1)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    with open(f\"model_all_{epoch}.pth\", \"wb\") as fp:\n",
    "        torch.save(model.state_dict(), fp)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_last = 1#str(num_epochs - 1)\n",
    "\n",
    "with open(f\"model_all_{epoch_last}.pth\", \"rb\") as fp:\n",
    "    best_state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "    model.load_state_dict(best_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rects(boxes):\n",
    "    rect = lambda x, y, w, h: patches.Rectangle((x, y), w - x, h - y, linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "    return [rect(box[0], box[1], box[2], box[3]) for box in boxes]\n",
    "\n",
    "def get_clazzes(labels, boxes, index2class):\n",
    "    return [{'x': box[0].item(), 'y': box[1].item() - 5.0, 's': index2class[label.item()], 'fontsize': 10}\n",
    "            for label, box in zip(labels, boxes)]\n",
    "\n",
    "def show_prediction(img, index2class, fig, ax):\n",
    "    pil_image = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
    "\n",
    "    ax.imshow(pil_image)\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "\n",
    "    for rect in get_rects(prediction[0]['boxes']):\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "\n",
    "    for label in get_clazzes(prediction[0]['labels'], prediction[0]['boxes'], index2class):\n",
    "        ax.text(**label)\n",
    "        \n",
    "        \n",
    "def get_prediction(dataset,  model):\n",
    "  #  img, _ = dataset[idx]\n",
    "\n",
    "    model.eval()\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "    \n",
    "    preds = []\n",
    "    for images in metric_logger.log_every(dataset, 100, header):\n",
    "\n",
    "        images = torch.stack([images[0][0].to(device), images[1][0].to(device), images[2][0].to(device) ], dim=0).unsqueeze(0)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "            outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        preds.append(outputs)\n",
    "\n",
    "        evaluator_time = time.time()\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        \n",
    "    return preds#img, prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "\n",
    "header = \"Test:\"\n",
    "dataloader_test = DataLoader(\n",
    "    test_data, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "predictions = get_prediction(dataloader_test, model) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "boxes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, i in enumerate(predictions):\n",
    "    for j in i:\n",
    "        for k in j['boxes'].cpu().detach().numpy():\n",
    "            boxes.append(k)\n",
    "            ids.append(test_ids[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(boxes), len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'file': ids, 'text': boxes}\n",
    "\n",
    "test = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names = test.file.values\n",
    "shapes = [cv2.imread(os.path.join(TEST_PATH, name), 0).shape for name in img_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['shapes']  = shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore(x, col_name):\n",
    "    \n",
    "    box = x[col_name].copy()\n",
    "\n",
    "    box[0] = int((box[0]/IMAGE_WIDTH) * x['shapes'][1])\n",
    "    box[1] = int((box[1]/IMAGE_HEIGHT) * x['shapes'][0])\n",
    "    box[2] = int((box[2]/IMAGE_WIDTH) * x['shapes'][1])\n",
    "    box[3] = int((box[3]/IMAGE_HEIGHT) * x['shapes'][0])    \n",
    "    \n",
    "    return box\n",
    "\n",
    "\n",
    "test['fixed_text'] = test.apply(lambda x: restore(x, 'text'), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['xmin'] = test.fixed_text.apply(lambda x: x[0])\n",
    "test['ymin'] = test.fixed_text.apply(lambda x: x[1])\n",
    "test['xmax'] = test.fixed_text.apply(lambda x: x[2])\n",
    "test['ymax'] = test.fixed_text.apply(lambda x: x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"test_first.csv\", index= None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test_first.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = open(TRAIN_INFO, \"r\")\n",
    "\n",
    "train = json_normalize(json.load(file_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['box'] = train.nums.apply(lambda x: list(chain.from_iterable(x[0]['box'])))\n",
    "train['text'] = train.nums.apply(lambda x: x[0]['text'])\n",
    "train['file'] = train.file.apply(lambda x: x.split(\"/\")[1])\n",
    "\n",
    "train.drop(['nums'], axis =1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.head(25631)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "img_names = train.file.values\n",
    "shapes = [cv2.imread(os.path.join(TRAIN_PATH, name), 0).shape for name in img_names]\n",
    "\n",
    "train['shapes'] = shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['xmin'] = train.apply(lambda x: min(x['box'][0], int(x['box'][6])), axis=1)\n",
    "train['xmax'] = train.apply(lambda x: max(x['box'][2], int(x['box'][4])), axis=1) \n",
    "train['ymin'] = train.apply(lambda x: min(x['box'][1], int(x['box'][3])), axis=1) \n",
    "train['ymax'] = train.apply(lambda x: max(x['box'][5], int(x['box'][7])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train[((train.ymax <= train.ymin + 5) | (train.xmax <= train.xmin + 5))]\n",
    "\n",
    "train = train[~((train.ymax <= train.ymin + 5) | (train.xmax <= train.xmin + 5))]\n",
    "train = train[~((train.xmin < 0 )| (train.ymin < 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['fixed_text'] = train.apply(lambda x: restore(x,  'box'), axis = 1)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test[((test.ymax <= test.ymin + 5) | (test.xmax <= test.xmin + 5))]\n",
    "\n",
    "test = test[~((test.ymax <= test.ymin + 5) | (test.xmax <= test.xmin + 5))]\n",
    "test = test[~((test.xmin < 0 )| (test.ymin < 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[~((train.xmin < 0 )| (train.ymin < 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"0123456789ABEKMHOPCTYX\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask(text):\n",
    "    \"\"\"Compute letter-digit mask of text.\n",
    "    Accepts string of text. \n",
    "    Returns string of the same length but with every letter replaced by 'L' and every digit replaced by 'D'.\n",
    "    e.g. 'E506EC152' -> 'LDDDLLDDD'.\n",
    "    Returns None if non-letter and non-digit character met in text.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    mask = []\n",
    "    for char in text:\n",
    "        if char in digits:\n",
    "            mask.append(\"D\")\n",
    "        elif char in ascii_uppercase:\n",
    "            mask.append(\"L\")\n",
    "        else:\n",
    "            return None\n",
    "    return \"\".join(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_in_alphabet(text, alphabet=abc):\n",
    "    \"\"\"Check if all chars in text come from alphabet.\n",
    "    Accepts string of text and string of alphabet. \n",
    "    Returns True if all chars in text are from alphabet and False else.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    for char in text:\n",
    "        if char not in alphabet:\n",
    "            return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(config):\n",
    "    \"\"\"Filter config keeping only items with correct text.\n",
    "    Accepts list of items.\n",
    "    Returns new list.\n",
    "    \"\"\"\n",
    "    config_filtered = []\n",
    "    for item in tqdm.tqdm(config):\n",
    "        text = item[\"text\"]\n",
    "        mask = compute_mask(text)\n",
    "        if check_in_alphabet(text) and (mask == \"LDDDLLDD\" or mask == \"LDDDLLDDD\"):\n",
    "            config_filtered.append({\"file\": item[\"file\"],\n",
    "                                    \"text\": item[\"text\"]})\n",
    "    return config_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionDataset(Dataset):\n",
    "    \"\"\"Class for training image-to-text mapping using CTC-Loss.\"\"\"\n",
    "\n",
    "    def __init__(self, df, alphabet=abc, transforms=None):\n",
    "        \"\"\"Constructor for class.\n",
    "        Accepts:\n",
    "        - config: list of items, each of which is a dict with keys \"file\" & \"text\".\n",
    "        - alphabet: string of chars required for predicting.\n",
    "        - transforms: transformation for items, should accept and return dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "        \"\"\"\n",
    "        super(RecognitionDataset, self).__init__()\n",
    "        self.df = df\n",
    "        self.alphabet = abc\n",
    "        self.image_names, self.texts = self._parse_root_()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def _parse_root_(self):\n",
    "        image_names, texts = [], []\n",
    "        #for item in self.train:\n",
    "        for index, item in self.df.iterrows():\n",
    "            #print(config)\n",
    "            \n",
    "            #print(item)\n",
    "            image_name = item[\"file\"]\n",
    "            text = item['text']\n",
    "            texts.append(text)\n",
    "            image_names.append(image_name)\n",
    "        return image_names, texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Return dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "        Image is a numpy array, float32, [0, 1].\n",
    "        Seq is list of integers.\n",
    "        Seq_len is an integer.\n",
    "        Text is a string.\n",
    "        \"\"\"\n",
    "\n",
    "        row = self.df[self.df.file == self.image_names[item]]\n",
    "\n",
    "        image = cv2.imread(os.path.join(TRAIN_PATH, self.image_names[item]))#[row['ymin']:row['ymax'], row['xmin']: row['xmax']].astype(np.float32) / 255.\n",
    "        #print(row['shapes'], int(row['ymin'].values[0]),int(row['ymax'].values[0]), int(row['xmin'].values[0]), int(row['xmax'].values[0]))\n",
    "        image = image[int(row['ymin'].values[0]):int(row['ymax'].values[0]), int(row['xmin'].values[0]): int(row['xmax'].values[0]) ]\n",
    "        iamge = image.astype(np.float32) / 255.\n",
    "\n",
    "        text = self.texts[item]\n",
    "        seq = self.text_to_seq(text)\n",
    "        seq_len = len(seq)\n",
    "        output = dict(image=image, seq=seq, seq_len=seq_len, text=text)\n",
    "        if self.transforms is not None:\n",
    "            output = self.transforms(output)\n",
    "        return output\n",
    "\n",
    "    def text_to_seq(self, text):\n",
    "        \"\"\"Encode text to sequence of integers.\n",
    "        Accepts string of text.\n",
    "        Returns list of integers where each number is index of corresponding characted in alphabet + 1.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        seq = [self.alphabet.find(c) + 1 for c in text]\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionDatasetTest(Dataset):\n",
    "    \"\"\"Class for training image-to-text mapping using CTC-Loss.\"\"\"\n",
    "\n",
    "    def __init__(self, test, alphabet=abc, transforms=None):\n",
    "        \"\"\"Constructor for class.\n",
    "        Accepts:\n",
    "        - config: list of items, each of which is a dict with keys \"file\" & \"text\".\n",
    "        - alphabet: string of chars required for predicting.\n",
    "        - transforms: transformation for items, should accept and return dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "        \"\"\"\n",
    "        super(RecognitionDatasetTest, self).__init__()\n",
    "        self.test = test\n",
    "        self.alphabet = abc\n",
    "        self.image_names, self.texts = self._parse_root_()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def _parse_root_(self):\n",
    "        image_names, texts = [], []\n",
    "        #for item in self.train:\n",
    "        for index, item in test.iterrows():\n",
    "\n",
    "            image_name = item[\"file\"]\n",
    "            text = item['text']\n",
    "            texts.append(text)\n",
    "            image_names.append(image_name)\n",
    "        return image_names, texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Return dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "        Image is a numpy array, float32, [0, 1].\n",
    "        Seq is list of integers.\n",
    "        Seq_len is an integer.\n",
    "        Text is a string.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        row = test.loc[item]\n",
    "        \n",
    "\n",
    "        image = cv2.imread(os.path.join(TEST_PATH, self.image_names[item]))#[row['ymin']:row['ymax'], row['xmin']: row['xmax']].astype(np.float32) / 255.\n",
    "        image = image[int(row['ymin']):int(row['ymax']), int(row['xmin']): int(row['xmax']) ]\n",
    "        iamge = image.astype(np.float32) / 255.\n",
    "\n",
    "        text = self.texts[item]\n",
    "        seq = self.text_to_seq(text)\n",
    "        seq_len = len(seq)\n",
    "        output = dict(image=image, seq=seq, seq_len=seq_len, text=text)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            output = self.transforms(output)\n",
    "        return output\n",
    "\n",
    "    def text_to_seq(self, text):\n",
    "        \"\"\"Encode text to sequence of integers.\n",
    "        Accepts string of text.\n",
    "        Returns list of integers where each number is index of corresponding characted in alphabet + 1.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        seq = [self.alphabet.find(c) + 1 for c in text]\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "\n",
    "    def __init__(self, size=(320, 64)):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, item):\n",
    "        \"\"\"Accepts item with keys \"image\", \"seq\", \"seq_len\", \"text\".\n",
    "        Returns item with image resized to self.size.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        item['image'] = cv2.resize(item['image'], self.size, interpolation=cv2.INTER_AREA)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Resize(size=(320, 64))\n",
    "dataset = RecognitionDataset(train, alphabet=abc, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n",
    "    Accepts list of dataset __get_item__ return values (dicts).\n",
    "    Returns dict with same keys but values are either torch.Tensors of batched images, sequences, and so.\n",
    "    \"\"\"\n",
    "    images, seqs, seq_lens, texts = [], [], [], []\n",
    "    for sample in batch:\n",
    "        images.append(torch.from_numpy(sample[\"image\"]).permute(2, 0, 1).float())\n",
    "        seqs.extend(sample[\"seq\"])\n",
    "        seq_lens.append(sample[\"seq_len\"])\n",
    "        texts.append(sample[\"text\"])\n",
    "    images = torch.stack(images)\n",
    "    seqs = torch.Tensor(seqs).int()\n",
    "    seq_lens = torch.Tensor(seq_lens).int()\n",
    "    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts}\n",
    "    return batch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n",
    "    Accepts list of dataset __get_item__ return values (dicts).\n",
    "    Returns dict with same keys but values are either torch.Tensors of batched images, sequences, and so.\n",
    "    \"\"\"\n",
    "    images, seqs, seq_lens, texts = [], [], [], []\n",
    "    for sample in batch:\n",
    "        images.append(torch.from_numpy(sample[\"image\"]).permute(2, 0, 1).float())\n",
    "        seqs.extend(sample[\"seq\"])\n",
    "        seq_lens.append(sample[\"seq_len\"])\n",
    "        texts.append(sample[\"text\"])\n",
    "    images = torch.stack(images)\n",
    "    seqs = torch.Tensor(seqs).int()\n",
    "    seq_lens = torch.Tensor(seq_lens).int()\n",
    "    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts}\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(Module):\n",
    "    \n",
    "    def __init__(self, input_size=(64, 320), output_len=20):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        \n",
    "        h, w = input_size\n",
    "        resnet = getattr(models, 'resnet34')(pretrained=True)\n",
    "        self.cnn = Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        self.pool = AvgPool2d(kernel_size=(h // 32, 1))        \n",
    "        self.proj = Conv2d(w // 32, output_len, kernel_size=1)\n",
    "  \n",
    "        self.num_output_features = self.cnn[-1][-1].bn2.num_features    \n",
    "    \n",
    "    def apply_projection(self, x):\n",
    "        \"\"\"Use convolution to increase width of a features.\n",
    "        Accepts tensor of features (shaped B x C x H x W).\n",
    "        Returns new tensor of features (shaped B x C x H x W').\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        x = x.permute(0, 3, 2, 1).contiguous()\n",
    "        x = self.proj(x)\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        return x\n",
    "   \n",
    "    def forward(self, x):\n",
    "        # Apply conv layers\n",
    "        features = self.cnn(x)\n",
    "        \n",
    "        # Pool to make height == 1\n",
    "        features = self.pool(features)\n",
    "        \n",
    "        # Apply projection to increase width\n",
    "        features = self.apply_projection(features)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 64, 320)\n",
    "y = feature_extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencePredictor(Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=True):\n",
    "        super(SequencePredictor, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes        \n",
    "        self.rnn = GRU(input_size=input_size,\n",
    "                       hidden_size=hidden_size,\n",
    "                       num_layers=num_layers,\n",
    "                       dropout=dropout,\n",
    "                       bidirectional=bidirectional)\n",
    "        \n",
    "        fc_in = hidden_size if not bidirectional else 2 * hidden_size\n",
    "        self.fc = Linear(in_features=fc_in,\n",
    "                         out_features=num_classes)\n",
    "    \n",
    "    def _init_hidden_(self, batch_size):\n",
    "        \"\"\"Initialize new tensor of zeroes for RNN hidden state.\n",
    "        Accepts batch size.\n",
    "        Returns tensor of zeros shaped (num_layers * num_directions, batch, hidden_size).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        num_directions = 2 if self.rnn.bidirectional else 1\n",
    "        return torch.zeros(self.rnn.num_layers * num_directions, batch_size, self.rnn.hidden_size)\n",
    "        \n",
    "    def _prepare_features_(self, x):\n",
    "        \"\"\"Change dimensions of x to fit RNN expected input.\n",
    "        Accepts tensor x shaped (B x (C=1) x H x W).\n",
    "        Returns new tensor shaped (W x B x H).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        x = x.squeeze(1)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._prepare_features_(x)\n",
    "        \n",
    "        batch_size = x.size(1)\n",
    "        h_0 = self._init_hidden_(batch_size)\n",
    "        h_0 = h_0.to(x.device)\n",
    "        x, h = self.rnn(x, h_0)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_predictor = SequencePredictor(input_size=512, \n",
    "                                       hidden_size=128, \n",
    "                                       num_layers=2, \n",
    "                                       num_classes=len(abc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 512, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sequence_predictor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(Module):\n",
    "    \n",
    "    def __init__(self, alphabet=abc,\n",
    "                 cnn_input_size=(64, 320), cnn_output_len=20,\n",
    "                 rnn_hidden_size=128, rnn_num_layers=2, rnn_dropout=0.3, rnn_bidirectional=True):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.alphabet = alphabet\n",
    "        self.features_extractor = FeatureExtractor(input_size=cnn_input_size, output_len=cnn_output_len)\n",
    "        self.sequence_predictor = SequencePredictor(input_size=self.features_extractor.num_output_features,\n",
    "                                                    hidden_size=rnn_hidden_size, num_layers=rnn_num_layers,\n",
    "                                                    num_classes=len(alphabet)+1, dropout=rnn_dropout,\n",
    "                                                    bidirectional=rnn_bidirectional)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features_extractor(x)\n",
    "        sequence = self.sequence_predictor(features)\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_string(pred, abc):\n",
    "    seq = []\n",
    "    for i in range(len(pred)):\n",
    "        label = np.argmax(pred[i])\n",
    "        seq.append(label - 1)\n",
    "    out = []\n",
    "    for i in range(len(seq)):\n",
    "        if len(out) == 0:\n",
    "            if seq[i] != -1:\n",
    "                out.append(seq[i])\n",
    "        else:\n",
    "            if seq[i] != -1 and seq[i] != seq[i - 1]:\n",
    "                out.append(seq[i])\n",
    "    out = ''.join([abc[c] for c in out])\n",
    "    return out\n",
    "\n",
    "def decode(pred, abc):\n",
    "    pred = pred.permute(1, 0, 2).cpu().data.numpy()\n",
    "    outputs = []\n",
    "    for i in range(len(pred)):\n",
    "        outputs.append(pred_to_string(pred[i], abc))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn = CRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 64, 320)\n",
    "y = crnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(y, abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTUALLY_TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn = CRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 8\n",
    "batch_size = 128\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda: 0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "crnn.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(crnn.parameters(), lr=3e-4, amsgrad=True, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_size = int(len(train) * 0.9)\n",
    "config_train = train[:train_size]\n",
    "config_val = train[train_size:]\n",
    "\n",
    "transforms = Resize(size=(320, 64))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RecognitionDataset(config_train, alphabet=abc, transforms=Resize())\n",
    "val_dataset = RecognitionDataset(config_val, alphabet=abc, transforms=Resize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, \n",
    "                              drop_last=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, \n",
    "                            drop_last=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_logic(x):\n",
    "    \"\"\"Filter config keeping only items with correct text.\n",
    "    Accepts list of items.\n",
    "    Returns new list.\n",
    "    \"\"\"\n",
    "\n",
    "    mask = compute_mask(x)\n",
    "    if check_in_alphabet(x) and (mask == \"LDDDLLDD\" or mask == \"LDDDLLDDD\" ):\n",
    "\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['index'] = test.file.apply(lambda x: int(x.split(\".\")[0]))\n",
    "test = test.sort_values(by=[  'index', 'xmin'], ascending=[ True, True]).drop(['index'], axis=1)#.head(n=20)\n",
    "\n",
    "\n",
    "test['file1'] = test['file']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m = test.groupby(['file1'])['text'].count().reset_index()\n",
    "\n",
    "data_m.columns = ['file', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = pd.merge(test, data_m, left_on= 'file', right_on='file', how='left' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if ACTUALLY_TRAIN:\n",
    "    for i, epoch in enumerate(range(num_epochs)):\n",
    "        crnn.train()\n",
    "        epoch_losses = []\n",
    "\n",
    "        for j, b in enumerate(tqdm.tqdm(train_dataloader, total=len(train_dataloader))):\n",
    "            images = b[\"image\"].to(device)\n",
    "            seqs_gt = b[\"seq\"]\n",
    "            seq_lens_gt = b[\"seq_len\"]\n",
    "\n",
    "            seqs_pred = crnn(images).cpu()\n",
    "            log_probs = log_softmax(seqs_pred, dim=2)\n",
    "            seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "\n",
    "            loss = ctc_loss(log_probs=log_probs,  # (T, N, C)\n",
    "                            targets=seqs_gt,  # N, S or sum(target_lengths)\n",
    "                            input_lengths=seq_lens_pred,  # N\n",
    "                            target_lengths=seq_lens_gt)  # N\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "        print(i, np.mean(epoch_losses))\n",
    "        crnn.eval()\n",
    "        best_val_loss = 999\n",
    "        val_losses = []\n",
    "        for i, b in enumerate(tqdm.tqdm(val_dataloader, total=len(val_dataloader))):\n",
    "            images = b[\"image\"].to(device)\n",
    "            seqs_gt = b[\"seq\"]\n",
    "            seq_lens_gt = b[\"seq_len\"]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                seqs_pred = crnn(images).cpu()\n",
    "            log_probs = log_softmax(seqs_pred, dim=2)\n",
    "            seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "            loss = ctc_loss(log_probs=log_probs,  # (T, N, C)\n",
    "                            targets=seqs_gt,  # N, S or sum(target_lengths)\n",
    "                            input_lengths=seq_lens_pred,  # N\n",
    "                            target_lengths=seq_lens_gt)  # N\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "        \n",
    "        if (best_val_loss  > np.mean(val_losses)):\n",
    "            with open(f\"model_{best_val_loss}.pth\", \"wb\") as fp:\n",
    "                torch.save(crnn.state_dict(), fp)\n",
    "                best_val_loss = np.mean(val_losses)\n",
    "                \n",
    "        print(np.mean(val_losses))\n",
    "        \n",
    "\n",
    "    with open(f\"model_{best_val_loss}.pth\", \"wb\") as fp:\n",
    "        torch.save(crnn.state_dict(), fp)\n",
    "\n",
    "\n",
    "        print(np.mean(val_losses))\n",
    "else:\n",
    "    image_train_log = cv2.imread(\"./resources/trwain_log.png\")\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    plt.imshow(image_train_log[:, :, ::-1], interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test.copy()\n",
    "\n",
    "test1['text'] = \"A232BC41\"\n",
    "\n",
    "test1 = test1[~((test1.ymin == test1.ymax) | (test1.xmin == test1.xmax) )].reset_index()\n",
    "\n",
    "dataset_test = RecognitionDatasetTest(test1, alphabet=abc, transforms=transforms)\n",
    "\n",
    "test_dataloader = DataLoader(dataset_test , \n",
    "                              batch_size=1, shuffle=False, num_workers=num_workers, pin_memory=True, \n",
    "                              drop_last=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "numbers = {}\n",
    "\n",
    "for i, b in enumerate(tqdm.tqdm(test_dataloader, total=len(test_dataloader))):\n",
    "    #print(b)\n",
    "    images = b[\"image\"].to(device)\n",
    "    seqs_gt = b[\"seq\"]\n",
    "    seq_lens_gt = b[\"seq_len\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        seqs_pred = crnn(images).cpu()\n",
    "    log_probs = log_softmax(seqs_pred, dim=2)\n",
    "    seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "    img_name = test[test.index == i]['file'].values[0]\n",
    "\n",
    "    cnt_img = test[test.index == i]['count'].values[0]\n",
    "\n",
    "    if (img_name not in numbers.keys()):\n",
    "\n",
    "        if cnt_img == 1:\n",
    "            numbers[img_name] = str(decode(log_probs, crnn.alphabet)[0])\n",
    "\n",
    "        else:\n",
    "            if check_data_logic(str(decode(log_probs, crnn.alphabet)[0])):\n",
    "                numbers[img_name] = str(decode(log_probs, crnn.alphabet)[0])\n",
    "\n",
    "    else:\n",
    "        if cnt_img == 1:\n",
    "            numbers[img_name] = str(decode(log_probs, crnn.alphabet)[0])\n",
    "\n",
    "        else:\n",
    "            if check_data_logic(str(decode(log_probs, crnn.alphabet)[0])):\n",
    "                numbers[img_name] = numbers[img_name] + \" \" +  str(decode(log_probs, crnn.alphabet)[0])\n",
    "\n",
    "\n",
    "ids_final = []\n",
    "num_final = []\n",
    "\n",
    "for i in numbers.keys():\n",
    "\n",
    "    ids_final.append(\"test/\" + i)\n",
    "    num_final.append(numbers[i])\n",
    "\n",
    "d = {'file_name': ids_final, 'plates_string':num_final}\n",
    "\n",
    "output = pd.DataFrame(data=d)\n",
    "\n",
    "\n",
    "sub = pd.read_csv(\"submission.csv\")\n",
    "\n",
    "final = pd.merge(sub.drop(['plates_string'], axis =1 ) , output, how = 'left', left_on = \"file_name\", right_on = \"file_name\" )\n",
    "\n",
    "final.to_csv(\"output.csv\", index = None)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
